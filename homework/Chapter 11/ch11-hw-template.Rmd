---
title: 'MECH481A6: Engineering Data Analysis in R'
subtitle: 'Chapter 11 Homework: Modeling' 
author: 'Genevieve Reyland-Slawson'
date: '`r format(Sys.time(), "%d %B, %Y")`'
output: html_document
---

```{r global-options, include=FALSE}
# set global options for figures, code, warnings, and messages
knitr::opts_chunk$set(fig.width=6, fig.height=4, fig.path="../figs/",
                      echo=TRUE, warning=FALSE, message=FALSE)
```

# Load packages

```{r load-packages, message=FALSE}
# load packages for current session
library(tidyverse) 
library(gridExtra) 
bodysize_csv <- "/Users/genevieve/Reyland-MECH476/data/bodysize.csv"
```

# Chapter 11 Homework

This homework will give you experience with OLS linear models and testing their assumptions.  

For this first problem set, we will examine issues of ***collinearity among predictor variables*** when fitting an OLS model with two variables. As you recall, assumption 3 from OLS regression requires there be no *collinearity* among predictor variables (the $X_i$'s) in a linear model.  The reason is that the model struggles to assign the correct $\beta_i$ values to each predictor when they are strongly correlated.   

## Question 1
Fit a series of three linear models on the `bodysize.csv` data frame using `lm()` with `height` as the dependent variable:  
  1. Model 1: use `waist` as the independent predictor variable:  
        - `formula = height ~ waist`   
  2. Model 2: use `mass` as the independent predictor variable:  
        - `formula = height ~ mass`  
  3. Model 3: use `mass + waist` as a linear combination of predictor variables:  
        - `formula = waist + mass`  
    
Report the coefficients for each of these models.  What happens to the sign and magnitude of the `mass` and `waist` coefficients when the two variables are included together?  Contrast that with the coefficients when they are used alone.

Evaluate assumption 3 about whether there is collinearity among these variables. Do you trust the coefficients from model 3 after having seen the individual coefficients reported in models 1 and 2?

   Looking at the summaries of the models, the intercepts and coefficients are not similar for any of the models. The coefficients from model 3 are combined of the first two models as it is comparing mass + waist, which means they're not trustworthy when comparing to model 1 and 2 coefficients. 

```{r ch11-homework-q1, echo=FALSE, include=FALSE}
bodysize_data <- readr::read_csv(file = bodysize_csv)
model1 <- lm(formula = height ~ waist, 
             data = bodysize_data)
coef_m1 <- coefficients(model1)

model2 <- lm(formula = height ~ mass, 
             data = bodysize_data)
coef_m2 <- coefficients(model2)

model3 <- lm(formula = height ~ waist + mass, 
             data = bodysize_data) 
coef_m3 <- coefficients(model3)

coef_m1
coef_m2
coef_m3
```

## Question 2
Create a new variable in the `bodysize` data frame using `dplyr::mutate`. Call this variable `volume` and make it equal to $waist^2*height$.  Use this new variable to predict `mass`.  

```{r ch11-homework-q2}
bodysize_data <- bodysize_data %>%
  dplyr::mutate(volume = waist^2 * height)
```

Does this variable explain more of the variance in `mass` from the NHANES data? How do you know? (hint: there is both *process* and *quantitative* proof here)
   Yes, this variance explains more as mass is a function of volume and density which means height and waist are less connected to the mass than the volume. 

```{r ch11-homework-q2a}
model4 <- lm(mass ~ volume,
             data = bodysize_data)
summary(model4)
```

Create a scatterplot of `mass` vs. `volume` to examine the fit.  Draw a fit line using `geom_smooth()`.

```{r ch11-homework-q2b}
q2b_plot <- ggplot(data = bodysize_data) + 
  geom_point(aes(x = volume,
                 y = mass), 
             alpha = 0.3, 
             color = "lightblue") + 
  geom_smooth(aes(x = volume,
                  y = mass), 
              method = "lm", 
              formula = "y ~ x",
              color = "black") + 
  labs(x = "Volume (cm^3)",
       y = "Mass (kg)",
       title = "Volume vs Mass") + 
  theme_bw()

q2b_plot

```

## Question 3
Load the `cal_aod.csv` data file and fit a linear model with `aeronet` as the independent variable and `AMOD` as the independent variable. 
```{r ch11-homework-q3}
# load data
caload_csv <- "/Users/genevieve/Reyland-MECH476/data/cal_aod.csv"
cal_aod <- readr::read_csv(file = caload_csv)

caload_model <- lm(formula = amod ~ aeronet, 
                   data = cal_aod)
```

Evaluate model assumptions 4-7 from the coursebook.  Are all these assumptions valid? 

```{r ch11-homework-q3a}
#assumption 4: mean of residuals is zero
caload_mean <- mean(caload_model$residuals)
caload_mean
```

```{r ch11-homework-q3b}
#assumption 5: residuals are normally distributed
q3b_model <- ggplot(data = caload_model$model,
                    aes(sample = caload_model$residuals)) + 
  geom_qq(alpha = 0.8,
          color = "maroon") + 
  geom_qq_line(color = "black") + 
  scale_y_continuous(limits = c(-0.1, 0.2)) +
  labs(title = "Mass vs Waist") + 
  theme_bw()
q3b_model
```

```{r ch11-homework-q3c}
#assumption 6: the error term is homoscedastic
q3c <- ggplot(data = caload_model$model) + 
  geom_point(aes(x = caload_model$fitted.values,
                 y = caload_model$residuals),
             alpha = 0.8,
             color = "skyblue") + 
  geom_hline(yintercept = 0) + 
  labs(x = "Residuals",
       y = "Fitted Values",
       title = "Mass vs Weight") + 
  theme_bw()
q3c
```

```{r ch11-homework-q3d}
#assumption 7: no autocorrelation among residuals
q3d <- cor(x = caload_model$residuals,
           y = caload_model$model[,2],
           method = "pearson")
q3d
```