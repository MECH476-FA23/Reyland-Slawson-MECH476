---
title: 'Chapter 11 Class Code'
subtitle: 'Code from class modeling chapter 11 concepts' 
author: 'Genevieve Reyland-Slawson'
date: '`r format(Sys.time(), "%d %B, %Y")`'
output: html_document
---

```{r global-options, include = FALSE}
# set global options for figures, code, warnings, and messages
knitr::opts_chunk$set(fig.width = 6, fig.height = 4, fig.path = "../figs/",
                      echo = TRUE, warning = FALSE, message = FALSE)
```

Model = a system of postulates, data, and inferences presented as a mathematical description of an entity or state of affiars
OLS = Ordinary Least Squares --> one of the most common ways to fit a linear regression. 
residuals that are larger have more impact on the line because when you square them, they are larger (1 squared is 10x less impactful than 10 squared)

Residuals are the errors in the model (y=mx+b), b is the error which are the residuals. The goal is to minimize the residuals to show the best thing. You should square the residuals so that it shows the negative values and they don't cancel out the positive residuals. 

Front-end asumptions: 
  - form of model is linear in coefficients --> parameter estimates (beta 0, beta 1, beta n) show up in final model as being added together 
    --> check by looking and see if beta is linear 
  - model is correctly specified 
  - no collinearity between predictor variables --> applies when model has more than one predictor (X1, X2, Xn) 
    --> check by running correlation analysis on pairs of independent variables (calculate pearson or spearman correlation coefficient) and create a correlation plot matris for all pairs and examine how they're correlated. 
    --> when two or more predictor variables are highly correlated, the model will not fit parameter estimates (beta values) well. (Not good)
    
Back-end assumptions: 
  -  mean of residuals is zero. --> can check by calculating the model residuals and take the mean
    --> if the mean is not very close to zero then there's been an error with the model implementation. It should be zero if the model lis set up and the OLS algorithm is implemented correctly. 
  - residuals are normally distributed --> can chack by creating a Q-Q plot about residuals
    --> iolation of normally assumption can affect the precision of estimates 
  - residuals are homoscedstic --> means to have equal variance 
    --> can check by creating a scatterplot of residuals, residuals should appear evenly scattered in both directions about zero with no change in magnitude as y increases. 
  - no residual autocorrelation --> autocorrelation means model is not properly specified 
    --> check by creating autocorrelation and partial correlation plots and perform a durbin-watson test.
  - no residual correlation with ind variables --> shows not properly specifed 

``` {r import packages} 
library(tidyverse)
library(ggplot2)
library(gridExtra)
```
# Plot Raw Data
``` {r plot raw data} 
body_csv <- "/Users/genevieve/Reyland-MECH476/data/bodysize.csv" 
body_data <- readr::read_csv(file = body_csv) %>%
  mutate(sex = gender - 1)
ggplot(data = body_data) + 
  geom_point(aes(x = waist, 
                 y = mass), 
                 alpha = 0.3)
```
clearly a strong relationship, looks very linear. First, we want to note, mass = density*volume = density*area*height

Then note, area = circumference^2 / 2pi if we approximate the human body as a sylinder. 

Then combine so mass = density*(cirfumfrence^2/2pi)*height. OR we could say that square root of mass is linearly related to body circumfrence. 

Converting mass to square root mass to examine the data set.
``` {r transform data} 
p1 <- ggplot(data = body_data) +
  geom_point(aes(x = waist, y = mass),
             alpha = 0.1,
             color = "maroon4") +
  ylab("Mass, kg") +
  xlab("Waist Circumference, cm") +
  theme_classic(base_size = 13)

p2 <- ggplot(data = body_data) +
  geom_point(aes(x = waist, y = sqrt_mass),
             alpha = 0.1,
             color = "royalblue2") +
  ylab(expression(sqrt(mass))) +
  xlab("Waist Circumference, cm") +
  theme_classic(base_size = 13)

grid.arrange(p1, p2, ncol = 2)
```

# The lm() Function
``` {r lm function}
# linear model = lm(), performs an OLS fit on any linear model specified
# lm() requires two args a formula and the data (y ~ x)

model1 <- lm(mass ~ waist, data = body_data)
model2 <- lm(sqrt_mass ~ waist, data = body_data) 
model1$coefficients
summary(model1)
```
Summary will be able to show how the model fits the data

  Option 1: One way is to add a geom_smooth() layer to the original scatter plot. The geom_smooth() function is a generic curve fitting algorithm for X, Y data. If we specify model = "lm" and formula = "y ~ x" as arguments, R will create identical linear models and plot those fits across the range of our data.
  
  Options 2: Another way would be to call geom_abline() and specify as arguments:
    slope = model1$coefficients[[2]] and
    intercept = model1$coefficients[[1]]
  Option 3: And yet a third way would be to use ggplot::stat_function() where we explicitly specify the model its coefficients as a linear function in R. You can learn more about each of these approaches in the help section or by typing ?stat_function into the Console.

``` {r see which is better}
# note how we can add layers to existing ggplot objects: added more to geom_smooth method, formula, color
p1.2 <- p1 + 
  geom_smooth(data = body_data,
              aes(x = waist, y = mass),
              method = "lm",
              formula = "y ~ x",
              color = "black")

p2.2 <- p2 +
  geom_abline(intercept = model2$coefficients[[1]],
              slope = model2$coefficients[[2]])

grid.arrange(p1.2, p2.2, ncol = 2)
```

``` {r lm function line}
# note how we can add layers to existing ggplot objects to see how the fit is
p1.2 <- p1 + 
  geom_smooth(data = data_18,
              aes(x = waist, y = mass),
              method = "lm",
              formula = "y ~ x",
              color = "black")

p2.2 <- p2 +
  geom_abline(intercept = model2$coefficients[[1]],
              slope = model2$coefficients[[2]])

grid.arrange(p1.2, p2.2, ncol = 2)
```

# Q-Q Plot
``` {r OLS diagnostics}
mean(model1$residuals)
mean(model2$residuals)

## Q-Q Plot to check for Assumption 5: the error term is normall distributed
p3 <- ggplot(data  = model1$model, aes(sample = model1$residuals)) +
  geom_qq(alpha = 0.1,
          color = "maroon4") +
  geom_qq_line(color = "grey") +
  ggtitle("Model 1: mass ~ waist") +
  theme_classic()

p4 <- ggplot(data  = model2$model, aes(sample = model2$residuals)) +
  geom_qq(alpha = 0.1,
          color = "royalblue2") +
  geom_qq_line(color = "grey") +
  ggtitle("Model 2: sqrt(mass) ~ waist") +
  theme_classic()

grid.arrange(p3, p4, ncol = 2)

# line is expected to be perfect, so the points should be on the line to be evenly distributed. The graphs show the expected over a normal distribution
```

# Homoscedastic Scatterplot
``` {r OLS diagnostics}
## checking assumption 6: the error term is homoscedastic
# shows resudal's size versus the y value 
p5 <- ggplot(data = model1$model) + 
  geom_point(aes(x = model1$fitted.values, y =model1$residuals),
             alpha = 0.25,
             color = "maroon3") +
  geom_hline(yintercept = 0) +
  theme_classic() +
  theme(aspect.ratio = 0.5)

p6 <- ggplot(data = model2$model)+ 
  geom_point(aes(x = model2$fitted.values, y =model2$residuals),
             alpha = 0.25,
             color= "royalblue2") +
  geom_hline(yintercept = 0) +
  theme_classic() +
  theme(aspect.ratio = 0.5)

grid.arrange(p5, p6, ncol = 1)
```

# Updated Model 
``` {r model3}
model3 <- lm(mass ~ waist + gender, data = body_data)
summary(model3)

# specify model more 
model4 <- lm(mass ~ height*waist^2 + gender + age, data = body_data)
summary(model4)
```

# Partial Autocorrelation Plot
``` {r OLS diagnostics}
# checking assumption 7: no autocorrelation among residuals
stats::pacf(model1$residuals,
           main = "Model 1 Partial Autocorrelation Plot")
stats::pacf(model2$residuals,
            main = "Model 2 Partial Autocorrelation Plot")

# neither plot show a strong degree of autocorrelation, make it better by dividing the models by sex
```

``` {r OLS diagnostics}
# checking assumption 8: residuals are not correlated with predictor variables 
cor(x = model1$residuals, 
    y = model1$model[,2],
    method = "pearson" )

cor(x = model2$residuals, 
    y = model1$model[,2],
    method = "pearson" )

# both very small numbers, model 2 is better specified (not better performing necessarily), but based on fit diagnostics and process knowledge, model 2 is better. Both explained ~80% of variance in dependent variable. 
```